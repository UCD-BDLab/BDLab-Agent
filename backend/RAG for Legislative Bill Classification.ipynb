{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a122b50a",
   "metadata": {},
   "source": [
    "# RAG for Legislative Bill Classification\n",
    "\n",
    "Manually classifying legislative text is a major bottleneck, especially for a label space exceeding 120 unique policy codes. This approach uses a Retrieval-Augmented Generation (RAG) framework to automate the process.\n",
    "\n",
    "The RAG is built on a high-quality ground truth corpus of over 300 bills annotated by domain experts. The RAG system works as follows:\n",
    "\n",
    "- **Embedding**: A `bge-large-en-v1.5` model creates dense vector representations of all coded paragraphs and definitions\n",
    "- **Indexing**: The embeddings are stored in a **FAISS** vector index for efficient, real-time similarity search\n",
    "- **Retrieval**: For any new paragraph, the system retrieves the most semantically relevant examples from the index\n",
    "- **Prompts**: Instead of overwhelming an LLM with all 120+ possible codes, the system builds a highly focused prompt with only a few relevant examples, overcoming context window limitations and improving accuracy\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "To start, I am conducting a comparative analysis to find the optimal balance of performance and cost by testing two distinct LLM strategies:\n",
    "\n",
    "### 1. Local Inference with Gemma 2 (9B)\n",
    "- **Model**: A locally-hosted instance of Google `Gemma 2 9B`, deployed with **4-bit precision** using `BitsAndBytesConfig`\n",
    "- **Hardware**: Runs locally on my workstation using an **NVIDIA RTX 4000 Ada Generation GPU (20GB)**\n",
    "- **Advantages**: This setup provides an unconstrained environment for experimentation, free from API latency, rate limits, or costs\n",
    "\n",
    "### 2. API-Based Inference with Gemini 2.5 Flash-Lite\n",
    "- **Model**: Google's `Gemini 2.5 Flash-Lite`, accessed via the Google AI Studio API\n",
    "- **Purpose**: Serves as a state-of-the-art performance baseline without requiring extensive local hardware\n",
    "- **Trade-Offs**: This approach is subject to API quotas, network latency, and potential costs\n",
    "\n",
    "The main objective is to quantify the performance differential between these two approaches to identify the most effective architecture for this task.\n",
    "\n",
    "## Next Steps: Multi-Agent Collaboration Framework\n",
    "\n",
    "The next phase will evolve the project from a comparative analysis into a multi-agent framework. The plan is to explore how collaboration between models can enhance classification accuracy and robustness.\n",
    "\n",
    "- **Goal**: Move beyond single-model classification to a system where multiple agents work together through a consensus or hierarchical structure.\n",
    "- **Agents**: The framework will orchestrate four models: two running locally (`Gemma 2 9B` and Microsoft `Phi-3-mini`) and two accessed via API (`Gemini 2.5 Flash-Lite` and `Gemini 2.0 Flash`)\n",
    "- **Next**: Research the specific strengths and weaknesses of each model to assign specialized roles, such as pre-processing, candidate generation, or final verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import glob\n",
    "import fitz\n",
    "import json\n",
    "import faiss\n",
    "import torch\n",
    "import jinja2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nested_output.json', 'r') as f:\n",
    "    nested_output = json.load(f)\n",
    "    \n",
    "#display(nested_output)\n",
    "\n",
    "PDF_DIRECTORY = \"Policy_PDFs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_documents = []\n",
    "code_names = []\n",
    "code_with_definitions = {}\n",
    "\n",
    "for category, subcategories in nested_output.items():\n",
    "    for subcategory, codes in subcategories.items():\n",
    "        for code_info in codes:\n",
    "\n",
    "            document_text = (\n",
    "                f\"Code Category: {category}. \"\n",
    "                f\"Code Subcategory: {subcategory}. \"\n",
    "                f\"Code Name: {code_info['Code Name']}. \"\n",
    "                f\"Definition: {code_info['Definition']}\"\n",
    "            )\n",
    "            #print(document_text)\n",
    "            code_names.append(code_info['Code Name'])\n",
    "            rag_documents.append(document_text)\n",
    "            code_with_definitions[code_info['Code Name']] = document_text\n",
    "\n",
    "#print(code_names)\n",
    "#print(rag_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = pd.read_csv(\"coded_paragraphs.csv\")\n",
    "\n",
    "save_paragraphs = {}\n",
    "rag_paragraphs = {}\n",
    "\n",
    "for idx, row in codebook.iterrows():\n",
    "    if len(row[\"paragraph\"]) < 1000 and len(row[\"paragraph\"]) > 150:\n",
    "        #row code is a string so we need to convert it to a list ex:\"['code1', 'code2']\"\n",
    "        as_list = row[\"codes\"].strip(\"[]\").replace(\"'\", \"\").split(\", \")\n",
    "        row[\"codes\"] = as_list\n",
    "\n",
    "        full_def = []\n",
    "        code_key = []\n",
    "        for c in row[\"codes\"]:\n",
    "            if c in code_with_definitions:\n",
    "                full_def.append(code_with_definitions[c])\n",
    "                code_key.append(c)\n",
    "\n",
    "        # join the full_def list into a single string\n",
    "        full_def_str = \" \".join(full_def)\n",
    "\n",
    "        save_paragraphs[row[\"document_name\"]] = {\"codes\":row[\"codes\"],\"codes_definition\":full_def,\"paragraph\":row[\"paragraph\"]}\n",
    "        rag_paragraphs[tuple(code_key)] = f\"{full_def_str} | Paragraph: {row[\"paragraph\"]}\"\n",
    "\n",
    "#print(len(save_paragraphs))\n",
    "\n",
    "#print(save_paragraphs.keys())\n",
    "display(len(rag_paragraphs))\n",
    "#save to json file\n",
    "# import json\n",
    "# with open(\"paragraph_with_codes_definitions.json\", \"w\") as f:\n",
    "#     json.dump(save_paragraphs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_model = \"backend/data/embeddings/bge-large\"\n",
    "embedder = SentenceTransformer(emd_model)\n",
    "\n",
    "definition_embeddings = embedder.encode(\n",
    "    list(rag_paragraphs.values()),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ").astype('float32')\n",
    "\n",
    "embedding_dimension = definition_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dimension)\n",
    "faiss.normalize_L2(definition_embeddings)\n",
    "index.add(definition_embeddings)\n",
    "\n",
    "# test query\n",
    "query = \"example code definition about affordable housing and state government the ppeople who need it will need to apply through a state agency\"\n",
    "\n",
    "query_embedding = embedder.encode([query], convert_to_numpy=True).astype('float32')\n",
    "faiss.normalize_L2(query_embedding)\n",
    "D, I = index.search(query_embedding, k=3)\n",
    "\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    #print(rag_paragraphs[list(rag_paragraphs.keys())[idx]])\n",
    "    print(f\"Similarity Score: {D[0][i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d015ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secrets.json', \"r\") as f:\n",
    "    secrets = json.load(f)\n",
    "\n",
    "#print(type(secrets))\n",
    "\n",
    "client = genai.Client(api_key=secrets[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_with_rag(paragraph):\n",
    "    # relevant context\n",
    "    query_embedding = embedder.encode([paragraph], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = index.search(query_embedding, k=7)\n",
    "\n",
    "    contexts = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        contexts.append(rag_paragraphs[list(rag_paragraphs.keys())[idx]])\n",
    "    \n",
    "    print(contexts)\n",
    "    #gather only the code names and definitions\n",
    "    codes_with_def = set()\n",
    "    just_para = \"\"\n",
    "    count = 1\n",
    "    for par in contexts:\n",
    "        parts = par.split(\" | \")\n",
    "        just_para += f\"\\nEXAMPLE {count}: \\n{parts[1]}\\nCodes Assigned: {parts[0]}\"\n",
    "\n",
    "        count -= 1\n",
    "        if count == 0:\n",
    "            break\n",
    "\n",
    "    for par in contexts:\n",
    "        try:\n",
    "            codes_string, _ = par.split(' | ')\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        pattern = r\"Code Name:\\s*(.+?)\\.\\s*Definition:\\s*(.+?)(?=Code Category:|$)\"\n",
    "        matches = re.findall(pattern, codes_string, re.DOTALL)\n",
    "\n",
    "        for code_name, definition in matches:\n",
    "            # captured groups\n",
    "            clean_name = code_name.strip()\n",
    "            clean_definition = definition.strip()\n",
    "\n",
    "            formatted_entry = f\"- {clean_name}. Definition: {clean_definition}\"\n",
    "            codes_with_def.add(formatted_entry)\n",
    "\n",
    "    # the final string\n",
    "    codes_context = \"CODES AND DEFINITIONS:\\n\" + \"\\n\".join(sorted(list(codes_with_def)))\n",
    "    #print(codes_context)\n",
    "\n",
    "    full_system_prompt = f\"\"\"\n",
    "    You will be shown some examples of code definitions and their corresponding paragraphs from US state policies/acts/bills.\n",
    "    Use these examples to get an idea of how to classify the paragraph into the appropriate codes.\n",
    "    \n",
    "    {just_para}\n",
    "\n",
    "    {codes_context}\n",
    "\n",
    "    Use the codes and their definitions to assign the most appropriate codes to the following paragraph.\n",
    "    Then, conclude with a comma-separated list of the codes names only.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=full_system_prompt),\n",
    "            contents=f\"Please code this paragraph: '{paragraph}'\"\n",
    "        )\n",
    "        print(response.text)\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Error]: {e}\")\n",
    "        return \"Error during API call.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "base = Path(\"backend/data/LLMs/Gemma-2-9B\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(str(base),quantization_config=quantization_config,device_map=\"auto\",local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(base),local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_directory(directory_path):\n",
    "\n",
    "    if not Path(directory_path).exists():\n",
    "        print(f\"Directory '{directory_path}' not found. Please create it and add your PDFs.\")\n",
    "        return []\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory_path, filename)\n",
    "            \n",
    "            try:\n",
    "                doc = fitz.open(filepath)\n",
    "                full_text = \"\".join(page.get_text() for page in doc)\n",
    "                doc.close()\n",
    "\n",
    "                if \"References\" in full_text:\n",
    "                    full_text = full_text[:full_text.index(\"References\")]\n",
    "\n",
    "                documents.append({\"page_content\": full_text,\"metadata\": {\"source\": filename}})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                \n",
    "    print(f\"Loaded {len(documents)} documents from '{directory_path}'\")\n",
    "    return documents\n",
    "\n",
    "def split_by_section(text):\n",
    "    pattern = r'(?=SECTION \\d+)'\n",
    "    clean_text = text.replace('\\n', ' ').replace('\\u00a0', ' ').replace('\\u00a7', '')\n",
    "    chunks = re.split(pattern, clean_text)\n",
    "\n",
    "    final_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        \n",
    "        if len(chunk) > 1000:\n",
    "            sub_chunks = textwrap.wrap(chunk, width=1000, break_long_words=False)\n",
    "            final_chunks.extend(sub_chunks)\n",
    "            \n",
    "        elif len(chunk) >= 100:\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_gemma(paragraph):\n",
    "    query_embedding = embedder.encode([paragraph], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = index.search(query_embedding, k=5)\n",
    "\n",
    "    contexts = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        contexts.append(rag_paragraphs[list(rag_paragraphs.keys())[idx]])\n",
    "    \n",
    "    codes_with_def = set()\n",
    "\n",
    "    for par in contexts:\n",
    "        try:\n",
    "            codes_string, _ = par.split(' | ')\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        pattern = r\"Code Name:\\s*(.+?)\\.\\s*Definition:\\s*(.+?)(?=Code Category:|$)\"\n",
    "        matches = re.findall(pattern, codes_string, re.DOTALL)\n",
    "\n",
    "        for code_name, definition in matches:\n",
    "            # captured groups\n",
    "            clean_name = code_name.strip()\n",
    "            clean_definition = definition.strip()\n",
    "\n",
    "            formatted_entry = f\"- {clean_name}. Definition: {clean_definition}\"\n",
    "            codes_with_def.add(formatted_entry)\n",
    "\n",
    "    # the final string\n",
    "    codes_context = \"Codes and definitions:\\n\" + \"\\n\".join(sorted(list(codes_with_def)))\n",
    "    #print(codes_context)\n",
    "    #load .txt file with the knowledge_base\n",
    "    with open(\"knowledge_base.txt\", \"r\") as f:\n",
    "        knowledge_base = f.read()\n",
    " \n",
    "    system_prompt = f\"\"\"\n",
    "    {knowledge_base}\n",
    "\n",
    "    ### Code Definitions\n",
    "    {codes_context}\n",
    "\n",
    "    ## YOUR TASK\n",
    "    Apply all rules and definitions from the KNOWLEDGE BASE above to the following paragraph. \n",
    "    Your entire output MUST BE a single, comma-separated list of the resulting 'Code Name's. \n",
    "    Do not explain your reasoning or show your work.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_prompt = f\"{system_prompt}\\n\\nParagraph to analyze: {paragraph}\"\n",
    "    messages = [{\"role\": \"user\", \"content\": combined_prompt}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs,max_new_tokens=300,do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|return|>\")])\n",
    "\n",
    "    raw_output = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return raw_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d09435",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE_LOCAL = \"classification_results_gemma_local.json\"\n",
    "def classify_gemma_local():\n",
    "    classification_results = defaultdict(list)\n",
    "    documents = load_pdfs_from_directory(PDF_DIRECTORY)\n",
    "    \n",
    "    for doc in documents:\n",
    "        filename = doc['metadata']['source']\n",
    "        print(f\"\\n NOw Processing {filename}\\n\")\n",
    "        \n",
    "        chunks = split_by_section(doc['page_content'])\n",
    "        print(f\"Found {len(chunks)} chunks to classify\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            predicted_codes = classify_with_gemma(chunk)\n",
    "            \n",
    "            paragraph_data = {\n",
    "                \"chunk_number\": i + 1,\n",
    "                \"text_snippet\": chunk,\n",
    "                \"predicted_codes\": predicted_codes\n",
    "            }\n",
    "            classification_results[filename].append(paragraph_data)\n",
    "            print(f\"Chunk {i+1}: {predicted_codes}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"\\nSaving Doc Results to: {RESULTS_FILE_LOCAL}\\n\")\n",
    "        with open(RESULTS_FILE_LOCAL, 'w') as f:\n",
    "            json.dump(classification_results, f, indent=4)\n",
    "\n",
    "classify_gemma_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f77e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESULTS_FILE = \"classification_results_gemini_master.json\"\n",
    "def classify_gemini_api():\n",
    "    classification_results = defaultdict(list)\n",
    "    documents = load_pdfs_from_directory(PDF_DIRECTORY)\n",
    "    \n",
    "    for doc in documents:\n",
    "        filename = doc['metadata']['source']\n",
    "        print(f\"\\n NOw Processing {filename}\\n\")\n",
    "        \n",
    "        chunks = split_by_section(doc['page_content'])\n",
    "        print(f\"Found {len(chunks)} chunks to classify.\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            predicted_codes_gemini = gemini_with_rag(chunk)\n",
    "            \n",
    "            paragraph_data = {\n",
    "                \"chunk_number\": i + 1,\n",
    "                \"text_snippet\": chunk,\n",
    "                \"predicted_codes\": predicted_codes_gemini\n",
    "            }\n",
    "            classification_results[filename].append(paragraph_data)\n",
    "            print(f\"Chunk {i+1}: {predicted_codes_gemini}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"\\nSaving Doc Results to: {RESULTS_FILE}\")\n",
    "        with open(RESULTS_FILE, 'w') as f:\n",
    "            json.dump(classification_results, f, indent=4)\n",
    "\n",
    "\n",
    "classify_gemini_api()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
